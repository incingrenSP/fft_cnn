{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745b857f-7650-4fba-90e1-217880ba52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fft as fft\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a17458d2-2e81-4764-aaf1-6f3448a232b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a84f894f-e7c5-4f31-a2f5-72cf67195214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import lmdb\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a91b1ff8-b239-4b50-8404-8060cebe5608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ImageHandler import *\n",
    "from src.FFTConv import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "952a1320-6293-4e3c-90bc-8a3a86003fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "REBUILD_DATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe7f7e0e-9465-4f89-a215-20d8d6649f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train dataset\n",
      "Loaded val dataset\n"
     ]
    }
   ],
   "source": [
    "lmdb_path = os.path.join('lmdb')\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    image_path = os.path.join('data', 'train_set')\n",
    "    train_data = ImageDataset(image_path=image_path, device=device, lmdb_path=lmdb_path, save_lmdb=True)\n",
    "    \n",
    "    image_path = os.path.join('data', 'val_set')\n",
    "    val_data = ImageDataset(image_path=image_path, device=device, lmdb_path=lmdb_path, save_lmdb=True, mode=\"val\")\n",
    "    REBUILD_DATA = False\n",
    "else:\n",
    "    train_data = ImageDataset(image_path=None, device=device, lmdb_path=lmdb_path, save_lmdb=False)\n",
    "    val_data = ImageDataset(image_path=None, device=device, lmdb_path=lmdb_path, save_lmdb=False, mode=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a7564d3-1216-4492-8bac-fff7a717b83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9961, 4200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6522ccb0-7ec7-4c56-9eb5-110b8849cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(train_data, batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_data, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e574177-1659-4d52-8dfb-8a7ffc783aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/312 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  torch.Size([32, 3, 128, 128])\n",
      "Label shape:  torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/132 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  torch.Size([32, 3, 128, 128])\n",
      "Label shape:  torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for images, labels in tqdm(train_dl):\n",
    "    labels = labels.squeeze().long()\n",
    "    print(\"Image shape: \", images.shape)\n",
    "    print(\"Label shape: \", labels.shape)\n",
    "    break\n",
    "\n",
    "for images, labels in tqdm(val_dl):\n",
    "    labels = labels.squeeze().long()\n",
    "    print(\"Image shape: \", images.shape)\n",
    "    print(\"Label shape: \", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fe29d35-b0ba-45a2-b652-4be8fb5fe85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GoogleNet\n",
    "learning_rate = 1e-2\n",
    "weight_decay = 1e-4\n",
    "apply_fft = False\n",
    "\n",
    "model = FFTGoogle(apply_fft=False, device=device)\n",
    "# model = FFTGoogle(apply_fft=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f26d9f38-cb21-42a3-bc42-b3ec408cfcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:  tensor([[0.1583, 0.0426, 0.0859]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Testing if model is working\n",
    "dummy_input = torch.randn(1,3,128,128).to(device)\n",
    "outputs = model(dummy_input)\n",
    "print(\"Output shape: \", outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62980b8f-377c-4f26-a65a-e51a9391f35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ") CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "print(optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df61b217-1677-4ea4-a087-464967812664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dl, val_dl, loss_fn, optimizer, scheduler, epochs, name, device):\n",
    "    best_acc = 0.0\n",
    "    tr_acc_list = []\n",
    "    val_acc_list = []\n",
    "    \n",
    "    # For mixed precision training\n",
    "    scaler = torch.amp.GradScaler(device)  \n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for images, labels in tqdm(train_dl):\n",
    "            # Move data to GPU\n",
    "            labels = labels.squeeze().long()\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            with torch.amp.autocast(device):\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs[0], labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Compute metrics\n",
    "            _, preds = torch.max(outputs[0], 1)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = running_corrects / total_samples\n",
    "        tr_acc_list.append(epoch_acc.cpu().item())\n",
    "        print(f\"Training Loss: {epoch_loss:0.6f}, Training Accuracy: {epoch_acc:0.6f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        val_loss, val_acc = validate(model, val_dl, loss_fn)\n",
    "        val_acc_list.append(val_acc.cpu().item())\n",
    "\n",
    "        # Save the best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            model.save_model_dict(os.path.join(\"models\", \"google\"), f\"{name}_model.pth\")\n",
    "            # torch.save(model.state_dict(), os.path.join(\"models\", \"google\", f\"{name}_model.pth\"))\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "    print('Training Complete.')\n",
    "    return tr_acc_list, val_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a08d2828-c8a4-47b8-bc0f-0b0ab872cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_dl, loss_fn):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_dl):\n",
    "            # Move data to GPU\n",
    "            labels = labels.squeeze().long()\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            # print(\"Output shape: \", outputs.shape)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Compute metrics\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        val_loss = running_loss / total_samples\n",
    "        val_acc = running_corrects / total_samples\n",
    "        print(f\"Validation Loss: {val_loss:0.6f}, Validation Accuracy: {val_acc:0.6f}\")\n",
    "\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3eaec969-469a-48ac-b43e-b9ac31bed22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 312/312 [01:56<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.195342, Training Accuracy: 0.497440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 132/132 [00:15<00:00,  8.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.728205, Validation Accuracy: 0.640000\n",
      "Training Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "name = \"reg_google\"\n",
    "tr_acc_list, val_acc_list = train(model, train_dl, val_dl, loss_fn, optimizer, scheduler, epochs=1, name=name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff8e512-3856-4200-98fd-8e0d564f7afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_fft:\n",
    "    tr_accuracy = np.array(tr_acc_list, dtype=np.float32)\n",
    "    val_accuracy = np.array(val_acc_list, dtype=np.float32)\n",
    "    np.save(os.path.join('models', 'tr_fft_accuracy.npy'), tr_accuracy)\n",
    "    np.save(os.path.join('models', 'val_fft_accuracy.npy'), val_accuracy)\n",
    "else:\n",
    "    tr_accuracy = np.array(tr_acc_list, dtype=np.float32)\n",
    "    val_accuracy = np.array(val_acc_list, dtype=np.float32)\n",
    "    np.save(os.path.join('models', 'tr_reg_accuracy.npy'), tr_accuracy)\n",
    "    np.save(os.path.join('models', 'val_reg_accuracy.npy'), val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7139454-3db8-43a3-bdb8-1e788319904e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda:nvidia",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
