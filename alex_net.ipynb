{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3cd6d99-3e63-4dd9-8881-8412153fe803",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T03:59:28.237291Z",
     "start_time": "2025-02-03T03:59:11.050211Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fft as fft\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ba4918-cc5c-4d47-8ae1-b8e8b538174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ff74d9a-b2fd-47b2-a955-2e8c3dfc6c98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T03:59:28.357971Z",
     "start_time": "2025-02-03T03:59:28.253251Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import lmdb\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b640dbb-6595-4604-8c01-c9e5714688ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.FFTConv import *\n",
    "from src.ImageHandler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2e6c79a-1512-408e-83b5-2eb10805c172",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T03:59:30.554130Z",
     "start_time": "2025-02-03T03:59:29.241877Z"
    }
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "REBUILD_DATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e42e952b-70b3-412c-906a-bf5eb243f3d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T04:16:56.826949Z",
     "start_time": "2025-02-03T04:12:25.512915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train dataset\n",
      "Loaded val dataset\n"
     ]
    }
   ],
   "source": [
    "lmdb_path = os.path.join('lmdb')\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    image_path = os.path.join('data', 'train_set')\n",
    "    train_data = ImageDataset(image_path=image_path, device=device, lmdb_path=lmdb_path, save_lmdb=True)\n",
    "    \n",
    "    image_path = os.path.join('data', 'val_set')\n",
    "    val_data = ImageDataset(image_path=image_path, device=device, lmdb_path=lmdb_path, save_lmdb=True, mode=\"val\")\n",
    "    REBUILD_DATA = False\n",
    "else:\n",
    "    train_data = ImageDataset(image_path=None, device=device, lmdb_path=lmdb_path, save_lmdb=False)\n",
    "    val_data = ImageDataset(image_path=None, device=device, lmdb_path=lmdb_path, save_lmdb=False, mode=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f6b302d-38c9-4b48-bfb1-1f06532b5e99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T04:28:55.654472Z",
     "start_time": "2025-02-03T04:28:55.641471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9959, 4200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "099eba50-6812-443c-9a7d-158d802e0eaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T04:28:56.957883Z",
     "start_time": "2025-02-03T04:28:56.947912Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dl = DataLoader(train_data, batch_size, shuffle=True, pin_memory=True)\n",
    "val_dl = DataLoader(val_data, batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66de5850-3dd2-44b7-9ac2-da6d90717ba6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T04:28:57.391172Z",
     "start_time": "2025-02-03T04:28:57.385188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  torch.Size([16, 3, 128, 128])\n",
      "Label shape:  torch.Size([16])\n",
      "Image shape:  torch.Size([16, 3, 128, 128])\n",
      "Label shape:  torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_dl:\n",
    "    labels = labels.squeeze().long()\n",
    "    print(\"Image shape: \", images.shape)\n",
    "    print(\"Label shape: \", labels.shape)\n",
    "    break\n",
    "\n",
    "for images, labels in val_dl:\n",
    "    labels = labels.squeeze().long()\n",
    "    print(\"Image shape: \", images.shape)\n",
    "    print(\"Label shape: \", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd9329e0-93b3-4278-a418-da2790e5af9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T04:28:59.223593Z",
     "start_time": "2025-02-03T04:28:58.583273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Layers replaced:  2\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5\n",
    "weight_decay = 1e-8\n",
    "\n",
    "model = FFTAlex(apply_fft=True, device=device, IMG_SIZE=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00b5e624-0a6d-4b21-bbc6-1413c7fb6ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9f1de52-d4da-4499-b723-aae1369cba2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:  tensor([[-0.0372,  0.0046,  0.0158]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Testing if model is working\n",
    "dummy_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "outputs = model(dummy_input)\n",
    "print(\"Output shape: \", outputs.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46972a1a-cfaa-4979-a1ff-8882816f9a24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T04:29:00.808188Z",
     "start_time": "2025-02-03T04:29:00.800208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 1e-05\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 1e-08\n",
      ") CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "print(optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdeb8546-9979-42ff-964f-957934cc5d85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T04:29:01.518272Z",
     "start_time": "2025-02-03T04:29:01.511291Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_dl, val_dl, loss_fn, optimizer, scheduler, epochs, name, device):\n",
    "    best_acc = 0.0\n",
    "    tr_acc_list = []\n",
    "    val_acc_list = []\n",
    "    \n",
    "    # Mixed precision training\n",
    "    scaler = torch.amp.GradScaler(device)  \n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for images, labels in tqdm(train_dl):\n",
    "            # Move data to GPU\n",
    "            labels = labels.squeeze().long()\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            with torch.amp.autocast(device):\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Compute metrics\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = running_corrects / total_samples\n",
    "        tr_acc_list.append(epoch_acc.cpu().item())\n",
    "        print(f\"Training Loss: {epoch_loss:0.6f}, Training Accuracy: {epoch_acc:0.6f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        val_loss, val_acc = validate(model, val_dl, loss_fn, device)\n",
    "        val_acc_list.append(val_acc.cpu().item())\n",
    "\n",
    "        # Save the best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            model.save_model_dict(os.path.join(\"models\", \"alex\"), f\"{name}_model.pth\")\n",
    "            # torch.save(model.state_dict(), os.path.join(\"models\", \"alex\", f\"{name}_model.pth\"))\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "    print('Training Complete.')\n",
    "    return tr_acc_list, val_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a338917-bc80-4858-b156-d3ea44083e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_dl, loss_fn, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_dl):\n",
    "            # Move data to GPU\n",
    "            labels = labels.squeeze().long()\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs.data, labels)\n",
    "\n",
    "            # Compute metrics\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        val_loss = running_loss / total_samples\n",
    "        val_acc = running_corrects / total_samples\n",
    "        print(f\"Validation Loss: {val_loss:0.6f}, Validation Accuracy: {val_acc:0.6f}\")\n",
    "\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b60b82c5-1385-4a6a-bf83-dc6f2b61fc07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T04:29:02.222244Z",
     "start_time": "2025-02-03T04:29:02.216260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 623/623 [30:15<00:00,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.062720, Training Accuracy: 0.402651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 263/263 [04:02<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.057646, Validation Accuracy: 0.478810\n",
      "Training Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "name = \"fft_alex\"\n",
    "tr_acc_list, val_acc_list = train(model, train_dl, val_dl, loss_fn, optimizer, scheduler, epochs=epochs, name=name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef07db8-bb4b-4278-8a2c-d230d63babe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T05:07:41.883936Z",
     "start_time": "2025-02-03T05:07:41.235486Z"
    }
   },
   "outputs": [],
   "source": [
    "tr_fft_accuracy = np.array(acc_list, dtype=np.float32)\n",
    "val_fft_accuracy = np.array(acc_list, dtype=np.float32)\n",
    "np.save(os.path.join('models', 'tr_fft_accuracy.npy'), tr_fft_accuracy)\n",
    "np.save(os.path.join('models', 'val_fft_accuracy.npy'), val_fft_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
