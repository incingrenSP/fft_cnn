{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b857f-7650-4fba-90e1-217880ba52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fft as fft\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f894f-e7c5-4f31-a2f5-72cf67195214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952a1320-6293-4e3c-90bc-8a3a86003fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db67ae8-7b1d-4659-93b6-ed1f38d618d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_path, device):\n",
    "        self.image_path = image_path\n",
    "        self.device = device\n",
    "        self.data = []\n",
    "        self.transform = v2.Compose([\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            v2.Normalize(mean=[0.5,], std=[0.5,]),\n",
    "            v2.Grayscale(num_output_channels=3)\n",
    "        ])\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        for idx, label in enumerate(os.listdir(os.path.join(self.image_path))):\n",
    "            print(os.path.join(self.image_path, label))\n",
    "            for img_file in tqdm(os.listdir(os.path.join(self.image_path, label))):\n",
    "                img = cv2.imread(os.path.join(self.image_path, label, img_file), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "                img = self.transform(img)\n",
    "\n",
    "                idx = torch.Tensor([idx])\n",
    "                self.data.append((img.to(device), idx.to(device)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f7e0e-9465-4f89-a215-20d8d6649f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "image_path = os.path.join('data', 'train_set')\n",
    "train_data = ImageDataset(image_path, device)\n",
    "train_dl = DataLoader(train_data, batch_size, shuffle=True)\n",
    "\n",
    "# image_path = os.path.join('data', 'test_set')\n",
    "# test_data = ImageDataset(image_path, device)\n",
    "# test_dl = DataLoader(train_data, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d32d40-0885-43b3-825d-eac7541ee03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTConvNet(nn.Module):\n",
    "    def __init__(self, conv_layer, fft_filter=None):\n",
    "        super().__init__()\n",
    "        self.conv_layer = conv_layer\n",
    "        self.fft_filter = fft_filter\n",
    "\n",
    "    def fft_filter_def(self, fft_x, height, width, device):\n",
    "        cht, cwt = height//2, width//2\n",
    "        mask_radius = 30\n",
    "        \n",
    "        mask = torch.zeros((height, width))\n",
    "        fy, fx = torch.meshgrid(torch.arange(0, height, device=device),\n",
    "                                torch.arange(0, width, device=device),\n",
    "                                indexing='ij')\n",
    "        mask_area = torch.sqrt((fx - cwt)**2 + (fy - cht)**2)\n",
    "\n",
    "        if self.fft_filter == 'high':\n",
    "            mask = (mask_area > mask_radius).float()\n",
    "        else:\n",
    "            mask = (mask_area <= mask_radius).float()\n",
    "        filtered_fft = fft_x * mask\n",
    "        \n",
    "        return filtered_fft\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        \n",
    "        # apply fft on input image\n",
    "        fft_x = fft.fft2(x)\n",
    "        fft_x = fft.fftshift(fft_x)\n",
    "\n",
    "        kernel_fft = fft.fft2(self.conv_layer.weight, s=(height, width))\n",
    "        kernel_fft = fft.fftshift(kernel_fft)\n",
    "\n",
    "        # apply fft filter (low pass)\n",
    "        if self.fft_filter is not None:\n",
    "            fft_x = self.fft_filter_def(fft_x, height, width, x.device)\n",
    "        \n",
    "        # perform element wise complex multiplcation\n",
    "        # fft_output = torch.sum(fft_x * kernel_fft, dim=2)\n",
    "\n",
    "        # trying einstein summation notation for implementing element wise complex multiplcation instead\n",
    "        fft_output = torch.einsum('bixy,oixy->boxy', fft_x, kernel_fft)\n",
    "        \n",
    "        # apply inverse fft\n",
    "        fft_output = fft.ifftshift(fft_output, dim=(-2,-1))\n",
    "        spatial_output = fft.ifft2(fft_output, dim=(-2,-1)).real\n",
    "        \n",
    "        # return output\n",
    "        return spatial_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f6ae4b-141f-494b-845e-6c4695cecbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_conv_layers(model):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            # if 'Conv2d_1a' in name or 'Conv2d_2a' in name or 'Conv2d_2b' in name:\n",
    "            # if 'Inception3' in name or 'Inception4' in name:\n",
    "            if 'Inception3' in name or 'Inception4' in name or 'Inception5' in name:\n",
    "                conv_fft = FFTConvNet(module, 'low')\n",
    "                setattr(model, name, conv_fft)\n",
    "        elif isinstance(module, nn.Sequential) or isinstance(module, nn.Module):\n",
    "            switch_conv_layers(module)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3626506-2304-4caf-aef5-804a5db66fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# googlenet\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.000001\n",
    "\n",
    "model = models.googlenet(weights=None, init_weights=True)\n",
    "\n",
    "# disable auxillary branches since we're working with size of images lower than 256\n",
    "model.aux1 = None\n",
    "model.aux2 = None\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 1024),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, 3),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "model = switch_conv_layers(model)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62980b8f-377c-4f26-a65a-e51a9391f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284f520-c6f0-4211-8904-f12fe99f0490",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff773fc-5651-473e-90ac-912f0559b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dl, loss_fn, optimizer, epochs):\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch: {epoch+1}')\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "        total_entries = 0\n",
    "\n",
    "        for images, labels in tqdm(train_dl):\n",
    "            labels = labels.squeeze().long()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = loss_fn(outputs[0], labels)\n",
    "            _, prediction = torch.max(outputs[0], 1)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += torch.sum((prediction == labels)).item()\n",
    "            total_entries += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total_entries\n",
    "        epoch_accuracy = running_corrects / total_entries\n",
    "        print(f'Epoch Loss: {epoch_loss:0.4f}\\tEpoch Accuracy:{epoch_accuracy:0.4f}')\n",
    "\n",
    "        if epoch_accuracy > best_accuracy:\n",
    "            best_accuracy = epoch_accuracy\n",
    "            torch.save(model.state_dict(), os.path.join('models', f'fft_google_model_e{epochs}.pth'))\n",
    "\n",
    "    print('Training Complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaec969-469a-48ac-b43e-b9ac31bed22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_dl, loss_fn, optimizer, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbae4359-6ce7-452d-9b94-3c6a4becbcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dl):\n",
    "    running_corrects = 0\n",
    "    running_accuracy = 0\n",
    "    total_entries = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_dl):\n",
    "            labels = labels.squeeze(0).long()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, prediction = torch.max(outputs[0], 1)\n",
    "\n",
    "            running_corrects += torch.sum((prediction == labels)).item()\n",
    "            total_entries += labels.size(0)\n",
    "        running_accuracy = running_corrects / total_entries\n",
    "        print(f'Model Accuracy: {running_accuracy:0.6f}')\n",
    "        print('Testing Complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cec1bea-ee3d-4173-ac0a-6d89230333f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77de599-8a77-47aa-8e41-6cdec9f09440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda:nvidia",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
