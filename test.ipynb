{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d2b32df-6707-4295-aa57-33e7cabb1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fft as fft\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e77428-9423-46f7-b1ba-3fbec1a063dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import lmdb\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe220fa-0226-445e-9464-b157b066d6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_lmdb_size(image_path):\n",
    "    total_size = 0\n",
    "    for label in os.listdir(image_path):\n",
    "        label_path = os.path.join(image_path, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            for img_file in os.listdir(label_path):\n",
    "                img_path = os.path.join(label_path, img_file)\n",
    "                total_size += os.path.getsize(img_path)\n",
    "    \n",
    "    buffer_factor = 1.5\n",
    "    return int(total_size * buffer_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f21f88-51a7-48b0-bfd7-65719351fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(\"data\", \"val_set\")\n",
    "map_size = estimate_lmdb_size(image_path)\n",
    "print(f\"Estimated LMDB size: {map_size / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6f78bb-9e36-438c-bb9b-c3ff4d421fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    transform = v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Resize((128,128))\n",
    "    ])\n",
    "    image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    image = transform(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1371ba07-39fd-4267-ab7e-5523c38304ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('..', 'image.jpg')\n",
    "image = load_image(path)\n",
    "fft_image = fft.fft2(image, dim=(-2,-1))\n",
    "fft_shift = fft.fftshift(fft_image, dim=(-2,-1))\n",
    "# fft_image = fft.fftshift(fft_image, dim=(-2,-1))\n",
    "magnitude = torch.log(torch.abs(fft_shift)+ 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a3132-f059-48fb-b2f3-97e1b26919ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(image.permute(1,2,0), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Magnitude Spectrum')\n",
    "plt.imshow(magnitude.permute(1,2,0), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ff60e-e687-459e-b81d-9958be9860bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = image.squeeze(0).shape\n",
    "crow, ccol = rows//2, cols//2\n",
    "radius = 50\n",
    "\n",
    "mask = torch.zeros((rows, cols))\n",
    "y, x = torch.meshgrid(torch.arange(0, rows), torch.arange(0, cols), indexing='ij')\n",
    "mask_area = torch.sqrt((x - ccol)**2 + (y - crow)**2)\n",
    "\n",
    "mask_h = (mask_area > radius).float()\n",
    "mask_l = (mask_area <= radius).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d11668-7995-4c6c-88c5-c44d35274ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_filtered_fft = fft_shift * mask_h\n",
    "high_filtered_image = torch.abs(fft.ifftshift(high_filtered_fft))\n",
    "magnitude_high_filtered_image = torch.log(torch.abs(high_filtered_image)+ 1)\n",
    "\n",
    "low_filtered_fft = fft_shift * mask_l\n",
    "low_filtered_image = torch.abs(fft.ifftshift(low_filtered_fft))\n",
    "magnitude_low_filtered_image = torch.log(torch.abs(low_filtered_image)+ 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0876851e-e949-4e01-a1c2-c93f7676503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "axes[0].set_title('Original')\n",
    "axes[0].imshow(image.permute(1,2,0), cmap='gray')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].set_title('FFT')\n",
    "axes[1].imshow(magnitude.permute(1,2,0), cmap='gray')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].set_title('Low Pass Filter')\n",
    "axes[2].imshow(magnitude_low_filtered_image.permute(1,2,0), cmap='gray')\n",
    "axes[2].axis('off')\n",
    "\n",
    "axes[3].set_title('High Pass Filter')\n",
    "axes[3].imshow(magnitude_high_filtered_image.permute(1,2,0), cmap='gray')\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580964fe-aa97-4895-9e36-fd0d5444682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTConvNet(nn.Module):\n",
    "    def __init__(self, conv_layer, fft_filter=None):\n",
    "        super().__init__()\n",
    "        self.conv_layer = conv_layer\n",
    "        self.fft_filter = fft_filter\n",
    "\n",
    "    def fft_filter_def(self, fft_x, height, width):\n",
    "        cht, cwt = height // 2, width // 2\n",
    "        mask_radius = 30\n",
    "\n",
    "        # Create a meshgrid for the mask\n",
    "        fy, fx = torch.meshgrid(\n",
    "            torch.arange(0, height, device=fft_x.device),\n",
    "            torch.arange(0, width, device=fft_x.device),\n",
    "            indexing='ij'\n",
    "        )\n",
    "        mask_area = torch.sqrt((fx - cwt) ** 2 + (fy - cht) ** 2)\n",
    "\n",
    "        # Create the mask based on the filter type\n",
    "        if self.fft_filter == 'high':\n",
    "            mask = (mask_area > mask_radius).float()\n",
    "        else:\n",
    "            mask = (mask_area <= mask_radius).float()\n",
    "\n",
    "        # Apply the mask to the FFT of the input\n",
    "        filtered_fft = fft_x * mask\n",
    "        return filtered_fft\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "\n",
    "        # Apply FFT on input image\n",
    "        fft_x = fft.fft2(x)\n",
    "        fft_x = fft.fftshift(fft_x)\n",
    "\n",
    "        # Apply FFT on the convolutional kernel\n",
    "        kernel_fft = fft.fft2(self.conv_layer.weight, s=(height, width))\n",
    "        kernel_fft = fft.fftshift(kernel_fft)\n",
    "\n",
    "        # Apply FFT filter (low-pass or high-pass)\n",
    "        if self.fft_filter is not None:\n",
    "            fft_x = self.fft_filter_def(fft_x, height, width)\n",
    "\n",
    "        # Perform element-wise complex multiplication\n",
    "        fft_output = fft_x * kernel_fft\n",
    "\n",
    "        # Apply inverse FFT\n",
    "        fft_output = fft.ifftshift(fft_output, dim=(-2, -1))\n",
    "        spatial_output = fft.ifft2(fft_output, dim=(-2, -1)).real\n",
    "\n",
    "        # Ensure the output has the same number of channels as the original convolution\n",
    "        spatial_output = spatial_output[:, :self.conv_layer.out_channels, :, :]\n",
    "\n",
    "        # Add bias (if applicable)\n",
    "        if self.conv_layer.bias is not None:\n",
    "            spatial_output += self.conv_layer.bias.view(1, -1, 1, 1)\n",
    "\n",
    "        # Return output\n",
    "        return spatial_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61925632-4c59-4dde-a68b-035de54ee289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_conv_layers(model):\n",
    "    for name, module in model.named_children():\n",
    "        if name.startswith('inception') and isinstance(module, nn.Conv2d):\n",
    "            print(name, module)\n",
    "            # fft_conv = FFTConvNet(module, 'low')\n",
    "            # setattr(model, name, fft_conv)\n",
    "        elif isinstance(module, nn.Sequential) or isinstance(module, nn.Module):\n",
    "            switch_conv_layers(module)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f307b8-d205-4cdc-92c0-aa855c0d9143",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.googlenet(weights='GoogLeNet_Weights.DEFAULT')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96798613-598e-4334-a87e-55c3d8dc1b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_layer(layer):\n",
    "    fft_conv = FFTConvNet(layer, 'low')\n",
    "    return fft_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f96fdaf-7734-48f5-80c0-65f8f0929dc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the output channels of the original Conv2d layer\n",
    "original_conv = model.inception3a.branch2[0].conv\n",
    "print(\"Original Conv2d output channels:\", original_conv.out_channels)\n",
    "\n",
    "# Replace the Conv2d layer with FFTConvNet\n",
    "fft_conv = change_layer(original_conv).to(device)\n",
    "model.inception3a.branch2[0].conv = fft_conv\n",
    "\n",
    "# Verify the output channels of the FFTConvNet layer\n",
    "print(\"FFTConvNet output channels:\", fft_conv.conv_layer.out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f6941c6-6519-4f05-9019-8cedcb60839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.alexnet(weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd728d13-0298-4cfd-8f5d-1fceb81a6e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (7): ReLU(inplace=True)\n",
       "  (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2dd15842-6338-40d9-8386-4026694c7fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.features = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2)),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "    nn.Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "    nn.Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    ")\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(9216, out_features=1024),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=1024, out_features=512),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(in_features=512, out_features=3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c292c318-d1e7-41ed-bb9e-b7794a4dece3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=1024, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Linear(in_features=512, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a17961a-201e-410f-80aa-acb0f9db9083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:  tensor([[-0.0209,  0.0312, -0.0249]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, 227, 227)\n",
    "outputs = model(dummy_input)\n",
    "print(\"Outputs: \", outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27efc941-41ef-4baf-b19e-13bb48769ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda:nvidia",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
